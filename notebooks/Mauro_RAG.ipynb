{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Construyendo mi primer RAG avanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante este notebook, vamos a ir paso a paso, hasta conseguir un RAG, que tenga informacion sobre dos pdfs completamente distintos, uno sobre arquitectura decoders y otro sobre modelos de Machine Learning para analisis financiero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Como hemos visto en clase, la arquitectura RAG se divide principalmente en dos fases:\n",
    "\n",
    "ETL (exctract, transform, load): Aqui es donde cargamos los archivos, los chunkeamos, y los metemos en la DB vectorial\n",
    "\n",
    "Inference-Time: Aqui es donde cogemos la pregunta del usuario, la codificamos y la comparamos con lo que tenemos en la db vectorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL (Exctract, Transform, Load)\n",
    "\n",
    "En este primer paso tenemos las siguientes tareas:\n",
    "\n",
    "1. Cargar el modelo de embeddings. Total libertad, pero os recomiendo utilizar uno de OpenAI, y asi no teneis que cargar nada en local\n",
    "https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "2. Lo pasamos por el flujo de procesamiento, podemos construirlo desde 0(detectar tablas, imagenes, texto, separarlo, guardar una referencia a la pagina del pdf a la que pertenecen...) o utilizar docling para pasar del pdf a documento docling con todo separado de forma estructural\n",
    "\n",
    "3. Procesamos el documento docling y lo pasamos todo a documentos langchain (pasamos las imagenes a texto, las tablas a markdown, y los textos grandes en chunks)\n",
    "\n",
    "4. Lo indexamos en la DB vectorial\n",
    "\n",
    "![](../img/ETL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar el modelo de embeddings. Total libertad, pero os recomiendo utilizar uno de OpenAI, y asi no teneis que cargar nada en local\n",
    "https://platform.openai.com/docs/guides/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# escribir aqui el codigo para cargar el modelo de embeddings de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lo pasamos por el flujo de procesamiento, podemos construirlo desde 0(detectar tablas, imagenes, texto, separarlo, guardar una referencia a la pagina del pdf a la que pertenecen...) o utilizar docling para pasar del pdf a documento docling con todo separado de forma estructural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# cargar los pdfs y procesarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Procesamos el documento docling y lo pasamos todo a documentos langchain (pasamos las imagenes a texto, las tablas a markdown, y los textos grandes en chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# codigo para procesar cada uno de los elementos del documento docling y pasarlos a documentos lanchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lo indexamos en la DB vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos qdrant y metemos todos los documentos que hemos procesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference-Time\n",
    "\n",
    "Cuando lleguemos aqui, ya deberiamos tener la db vectorial poblada con todos los vectores de ambos pdfs\n",
    "\n",
    "Ahora tenemos que construir el sistema que recupere los vectores relevantes en base a la query del usuario y los introduzca en el prompt del modelo, para ello:\n",
    "\n",
    "1. Cargamos el LLM, podemos utilizar OpenAI, para sencillez, o la API de groq que es gratuita\n",
    "\n",
    "2. Construimos el retriever:\n",
    "   - Podemos hacerlo manualmente: codificar la query del usuario con el modelo de embeddings que hemos utilzado al principio, y hacer la comparacion con los vectores de la db\n",
    "   - Podemos automatizarlo, utilizando langchain y convirtiendo nuestra db de qdrant en un objeto retriever de langchain\n",
    "\n",
    "3. Construimos el prompt template que introduce tanto la query original del usuario como los documetos recibidos dentro de un mismo prompt\n",
    "\n",
    "4. Hacemos una pregunta y se la enviamos al retriever, con los documentos recuperados llamamos al modelo rellenando el prompt template.\n",
    "\n",
    "5. (opcional) Probamos a incorporar reranking \n",
    "\n",
    "![](../img/Inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargamos el LLM, podemos utilizar OpenAI, para sencillez, o la API de groq que es gratuita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Codigo para cargar el modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construimos el retriever:\n",
    "   - Podemos hacerlo manualmente: codificar la query del usuario con el modelo de embeddings que hemos utilzado al principio, y hacer la comparacion con los vectores de la db\n",
    "   - Podemos automatizarlo, utilizando langchain y convirtiendo nuestra db de qdrant en un objeto retriever de langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Codigo para construir el retriever, o la pipeline de retrieving si lo decidimos hacer manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construimos el prompt template que introduce tanto la query original del usuario como los documetos recibidos dentro de un mismo prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# codigo para el prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hacemos una pregunta y se la enviamos al retriever, con los documentos recuperados llamamos al modelo rellenando el prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Le pasamos al modelo la query del usuario y los documentos recuperados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (opcional) Probamos a incorporar reranking "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
